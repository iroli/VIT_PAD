    1. Объясните концепцию механизмов внимания в нейронных сетях для задач обработки естественного языка. Как они устраняют ограничения RNN при обработке долгосрочных зависимостей?
    2. Опишите различия между контролируемым и неконтролируемым обучением в обработке естественного языка. Приведите примеры задач, которые обычно подходят для каждого подхода.
    3. Объясните роль эмбедингов в обработке естественного языка. Сравните и сопоставьте эмбединги Wordec и GloVe, подчеркнув их сильные и слабые стороны.
    4. Каковы основные проблемы при обработке неоднозначности в языке? Как модели обработки естественного языка решают эти проблемы, особенно в таких задачах, как устранение неоднозначности смысла слова?
    5. Объясните концепцию переноса обучения в обработке естественного языка. Как это улучшает производительность модели, особенно при работе с ограниченными данными обучения?
    6. Сравните и сопоставьте архитектуры BERT и GPT. Как их выбор дизайна влияет на их производительность в различных задачах обработки естественного языка?
    7. Обясните важность контекста в задачах обработки естественного языка. Как модели, подобные Transformers, захватывают контекстную информацию более эффективно, чем традиционные RNN?
    8. Опишите процесс нормализации текста и его значение в конвейерах обработки естественного языка. Каковы некоторые распространенные методы, используемые для нормализации текста?
    9. Что такое perplexity в языковых моделях. Как она используется для оценки этих моделей?
    10. Каковы основные различия между стеммингом и лемматизацией? Предоставьте сценарии, в которых один из них может быть предпочтительнее другого.
    11. Объясните концепцию моделирования тем и его применение в обработке естественного языка. Как работает скрытое распределение Дирихле (LDA)? Что такое BertTopic?
    12. Опишите процесс распознавания именованных сущностей (NER) и его важность в обработке естественного языка. Как современные модели, подобные Transformers, улучшают задачи NER?
    13. Объясните роль маркировки частей речи в NLP. Как она обычно реализуется и каковы ее приложения? 
    14. Объясните пять фаз обработки NLP (синтаксический, семантический, дискурсивный, прагматический и морфологический анализ) и их взаимозависимости.
    15. Как анализ зависимостей способствует пониманию грамматической структуры предложений? Сравните подходы, основанные на правилах, и подходы машинного обучения.
    16. Объясните разрешение кореферентности и его влияние на связность в длинных текстах. Какие методы на основе трансформаторов улучшают эту задачу?
    17. Опишите состязательные  атаки, характерные для моделей NLP. Какие существуют механизмы защиты?
    18. Когда регулярных выражений недостаточно для сложного сопоставления текстовых шаблонов? Приведите примеры, требующие вероятностных методов.
    19. Сравните моделирование маскированного языка (стиль BERT) с моделированием каузального языка (стиль GPT) с точки зрения целей предварительной подготовки и последующих приложений.
    20. Объясните метрики  BLEU/ROUGE/BertScore.
    21. Объясните метрики  точность, полнота, f1 мера.
    22. Сравните представления   one hot encoding,  Bag-of-Words, TF-IDF.

.


    1. Что такое Language AI и чем оно отличается от общего понятия AI?
    2. Какие ключевые этапы в истории развития Language AI?
    3. Объясните подход Bag-of-Words и его ограничения.
    4. Что такое эмбеддинги и как они улучшают представление текста?
    5. В чем отличие word2vec от bag-of-words?
    6. Что такое нейронная сеть и какие её ключевые элементы?
    7. Объясните идею контекстной обработки в RNN.
    8. Что такое attention-механизм и почему он важен?
    9. Как работает self-attention в трансформерах?
    10. Чем отличается self-attention от обычного attention?
    11. В чем суть архитектуры Transformer?
    12. Как устроены encoder и decoder в Transformer?
    13. Что такое маскирование в декодере трансформера?
    14. Какие существуют типы моделей: encoder-only, decoder-only, encoder-decoder?
    15. Опишите архитектуру BERT.
    16. Как работает masked language modeling?
    17. Чем GPT-архитектура отличается от BERT?
    18. Что означает термин "autoregressive model"?
    19. Что такое контекстное окно (context window)?
    20. В чем заключается основная идея статьи "Attention is All You Need"?
    21. Как использовать LLM без дообучения?
    22. Как реализуется текстовая классификация с помощью LLM?
    23. В чем суть кластеризации и тематического моделирования?
    24. Как работают sentence embeddings?
    25. Что такое семантический поиск и embedding space?
    26. Объясните концепцию retrieval-augmented generation (RAG).
    27. Что такое семантический поиск?
    28. Как работает генерация текста в LLM?
    29. Что такое sampling, temperature, top-k, top-p в генерации текста?
    30. Как LLM могут быть адаптированы для визуального ввода (мультимодальность)?
    31. Чем отличаются supervised и unsupervised подходы при использовании LLM?
    31. Что такое pretraining и fine-tuning в контексте LLM?
    32. Зачем нужны foundation models?
    33. Как дообучается embedding model?
    34. Как fine-tune'ить BERT для классификации?
    35. Какие подходы существуют для дообучения генеративных моделей?
    36. Чем отличается instruction tuning от general fine-tuning?
    37. Объясните процесс alignment и его значение.
    38. Что такое transfer learning и как он применяется в LLM?
    41. Какие этические риски связаны с использованием LLM?
    42. Что такое bias в языковых моделях?

